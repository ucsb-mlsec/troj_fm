
-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 11:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                   4       
data parallel size:                                           4       
model parallel size:                                          1       
batch size per GPU:                                           4       
params per gpu:                                               131.34 M
params of model = params per GPU * mp_size:                   131.34 M
fwd MACs per GPU:                                             27337.47 GMACs
fwd flops per GPU:                                            54676.73 G
fwd flops of model = fwd flops per GPU * mp_size:             54676.73 G
fwd latency:                                                  375.04 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          145.79 TFLOPS
bwd latency:                                                  442.45 ms
bwd FLOPS per GPU = 2.0 * fwd flops per GPU / bwd latency:    247.15 TFLOPS
fwd+bwd FLOPS per GPU = 3.0 * fwd flops per GPU / (fwd+bwd latency):   200.65 TFLOPS
step latency:                                                 173.9 ms
iter latency:                                                 991.4 ms
FLOPS per GPU = 3.0 * fwd flops per GPU / iter latency:       165.45 TFLOPS
samples/second:                                               16.14   

----------------------------- Aggregated Profile per GPU -----------------------------
Top 3 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'LlamaForCausalLM': '131.34 M'}
    MACs        - {'LlamaForCausalLM': '27337.47 GMACs'}
    fwd latency - {'LlamaForCausalLM': '373.87 ms'}
depth 1:
    params      - {'LlamaModel': '131.34 M', 'Linear': '0'}
    MACs        - {'LlamaModel': '26800.6 GMACs', 'Linear': '536.87 GMACs'}
    fwd latency - {'LlamaModel': '370.13 ms', 'Linear': '2.19 ms'}
depth 2:
    params      - {'Embedding': '131.07 M', 'ModuleList': '262.14 k', 'LlamaRMSNorm': '4.1 k'}
    MACs        - {'ModuleList': '26800.6 GMACs', 'Embedding': '0 MACs', 'LlamaRMSNorm': '0 MACs'}
    fwd latency - {'ModuleList': '356.91 ms', 'LlamaRMSNorm': '300.17 us', 'Embedding': '262.74 us'}
depth 3:
    params      - {'LlamaDecoderLayer': '262.14 k'}
    MACs        - {'LlamaDecoderLayer': '26800.6 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '356.91 ms'}
depth 4:
    params      - {'LlamaRMSNorm': '262.14 k', 'LlamaAttention': '0', 'LlamaMLP': '0'}
    MACs        - {'LlamaMLP': '17729.62 GMACs', 'LlamaAttention': '9070.97 GMACs', 'LlamaRMSNorm': '0 MACs'}
    fwd latency - {'LlamaAttention': '151.61 ms', 'LlamaMLP': '131.7 ms', 'LlamaRMSNorm': '19.29 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

LlamaForCausalLM(
  131.34 M, 100.00% Params, 27337.47 GMACs, 100.00% MACs, 373.87 ms, 100.00% latency, 146.24 TFLOPS, 
  (model): LlamaModel(
    131.34 M, 100.00% Params, 26800.6 GMACs, 98.04% MACs, 370.13 ms, 99.00% latency, 144.82 TFLOPS, 
    (embed_tokens): Embedding(131.07 M, 99.80% Params, 0 MACs, 0.00% MACs, 262.74 us, 0.07% latency, 0.0 FLOPS, 32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 13.36 ms, 3.57% latency, 125.35 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 5.49 ms, 1.47% latency, 103.26 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 369.07 us, 0.10% latency, 372.39 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.23 us, 0.09% latency, 398.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.71 us, 0.09% latency, 397.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.95 us, 0.09% latency, 397.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 122.79 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.61 ms, 1.23% latency, 240.4 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 837.33 us, 0.22% latency, 441.13 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 830.41 us, 0.22% latency, 444.8 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 833.51 us, 0.22% latency, 443.15 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 120.4 us, 0.03% latency, 187.24 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 306.37 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 306.13 us, 0.08% latency, 0.0 FLOPS, )
      )
      (1): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 13.35 ms, 3.57% latency, 125.45 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 5.54 ms, 1.48% latency, 102.26 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 350.71 us, 0.09% latency, 391.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.42 us, 0.09% latency, 396.74 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 351.19 us, 0.09% latency, 391.35 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 349.04 us, 0.09% latency, 393.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 121.83 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.61 ms, 1.23% latency, 240.37 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 826.36 us, 0.22% latency, 446.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 823.74 us, 0.22% latency, 448.4 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 823.26 us, 0.22% latency, 448.66 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 108.48 us, 0.03% latency, 207.82 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 305.41 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 306.13 us, 0.08% latency, 0.0 FLOPS, )
      )
      (2): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 13.31 ms, 3.56% latency, 125.83 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 5.46 ms, 1.46% latency, 103.76 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 348.33 us, 0.09% latency, 394.57 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 344.99 us, 0.09% latency, 398.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 347.61 us, 0.09% latency, 395.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 348.57 us, 0.09% latency, 394.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 119.69 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.62 ms, 1.24% latency, 239.92 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 826.12 us, 0.22% latency, 447.11 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 823.5 us, 0.22% latency, 448.53 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 822.07 us, 0.22% latency, 449.32 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 109.91 us, 0.03% latency, 205.11 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 302.55 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 302.31 us, 0.08% latency, 0.0 FLOPS, )
      )
      (3): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 13.54 ms, 3.62% latency, 123.76 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 5.42 ms, 1.45% latency, 104.53 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 348.57 us, 0.09% latency, 394.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.42 us, 0.09% latency, 396.74 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 344.51 us, 0.09% latency, 398.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 348.57 us, 0.09% latency, 394.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 119.45 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.6 ms, 1.23% latency, 241.16 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 825.64 us, 0.22% latency, 447.37 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 821.11 us, 0.22% latency, 449.84 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 821.59 us, 0.22% latency, 449.58 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 105.62 us, 0.03% latency, 213.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 303.98 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 304.22 us, 0.08% latency, 0.0 FLOPS, )
      )
      (4): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 13.86 ms, 3.71% latency, 120.9 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 5.69 ms, 1.52% latency, 99.56 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 351.91 us, 0.09% latency, 390.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 344.75 us, 0.09% latency, 398.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 344.51 us, 0.09% latency, 398.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 348.57 us, 0.09% latency, 394.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 117.78 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.89 ms, 1.31% latency, 226.76 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 823.74 us, 0.22% latency, 448.4 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 820.64 us, 0.22% latency, 450.1 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 822.78 us, 0.22% latency, 448.92 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 105.38 us, 0.03% latency, 213.93 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 303.98 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 303.27 us, 0.08% latency, 0.0 FLOPS, )
      )
      (5): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.64 ms, 3.11% latency, 143.93 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.65 ms, 1.24% latency, 122.05 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.95 us, 0.09% latency, 397.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 349.52 us, 0.09% latency, 393.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 348.09 us, 0.09% latency, 394.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 347.14 us, 0.09% latency, 395.92 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 119.69 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.4 ms, 1.18% latency, 251.86 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 822.31 us, 0.22% latency, 449.18 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 818.25 us, 0.22% latency, 451.41 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 822.54 us, 0.22% latency, 449.05 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 106.57 us, 0.03% latency, 211.54 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 303.51 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 299.93 us, 0.08% latency, 0.0 FLOPS, )
      )
      (6): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.08 ms, 2.96% latency, 151.22 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.69 ms, 1.25% latency, 120.98 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 348.33 us, 0.09% latency, 394.57 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 349.76 us, 0.09% latency, 392.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 348.81 us, 0.09% latency, 394.03 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.66 us, 0.09% latency, 396.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 116.11 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.11 ms, 1.10% latency, 269.52 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 822.54 us, 0.22% latency, 449.05 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 817.06 us, 0.22% latency, 452.07 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 822.78 us, 0.22% latency, 448.92 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 112.53 us, 0.03% latency, 200.33 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 303.03 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 302.31 us, 0.08% latency, 0.0 FLOPS, )
      )
      (7): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.03 ms, 2.95% latency, 151.82 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.63 ms, 1.24% latency, 122.38 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 350.0 us, 0.09% latency, 392.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 348.09 us, 0.09% latency, 394.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.71 us, 0.09% latency, 397.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.71 us, 0.09% latency, 397.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 118.26 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.14 ms, 1.11% latency, 267.58 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 826.36 us, 0.22% latency, 446.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 817.54 us, 0.22% latency, 451.8 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 828.98 us, 0.22% latency, 445.57 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 105.14 us, 0.03% latency, 214.42 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 302.08 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 299.22 us, 0.08% latency, 0.0 FLOPS, )
      )
      (8): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.11 ms, 2.97% latency, 150.83 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.63 ms, 1.24% latency, 122.47 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.95 us, 0.09% latency, 397.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 347.38 us, 0.09% latency, 395.65 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 347.14 us, 0.09% latency, 395.92 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.66 us, 0.09% latency, 396.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 118.02 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.21 ms, 1.13% latency, 263.17 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 816.35 us, 0.22% latency, 452.46 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 817.06 us, 0.22% latency, 452.07 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 816.58 us, 0.22% latency, 452.33 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 105.38 us, 0.03% latency, 213.93 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 301.6 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 299.69 us, 0.08% latency, 0.0 FLOPS, )
      )
      (9): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.0 ms, 2.94% latency, 152.31 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.64 ms, 1.24% latency, 122.22 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.18 us, 0.09% latency, 397.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 347.61 us, 0.09% latency, 395.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 351.91 us, 0.09% latency, 390.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.95 us, 0.09% latency, 397.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 123.74 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.1 ms, 1.10% latency, 270.06 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 817.54 us, 0.22% latency, 451.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 819.21 us, 0.22% latency, 450.88 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 816.35 us, 0.22% latency, 452.46 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 106.81 us, 0.03% latency, 211.07 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 300.88 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 299.45 us, 0.08% latency, 0.0 FLOPS, )
      )
      (10): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.03 ms, 2.95% latency, 151.91 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.66 ms, 1.25% latency, 121.66 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.66 us, 0.09% latency, 396.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 348.09 us, 0.09% latency, 394.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.66 us, 0.09% latency, 396.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.23 us, 0.09% latency, 398.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 119.69 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.12 ms, 1.10% latency, 269.0 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 816.35 us, 0.22% latency, 452.46 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 816.58 us, 0.22% latency, 452.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 819.21 us, 0.22% latency, 450.88 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 106.1 us, 0.03% latency, 212.49 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 301.84 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 299.22 us, 0.08% latency, 0.0 FLOPS, )
      )
      (11): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.09 ms, 2.97% latency, 151.11 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.65 ms, 1.24% latency, 121.93 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.95 us, 0.09% latency, 397.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 348.57 us, 0.09% latency, 394.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.9 us, 0.09% latency, 396.19 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.9 us, 0.09% latency, 396.19 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 119.45 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.14 ms, 1.11% latency, 267.41 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 821.11 us, 0.22% latency, 449.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 825.64 us, 0.22% latency, 447.37 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 817.78 us, 0.22% latency, 451.67 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 106.33 us, 0.03% latency, 212.01 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 302.31 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 299.69 us, 0.08% latency, 0.0 FLOPS, )
      )
      (12): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.07 ms, 2.96% latency, 151.36 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.64 ms, 1.24% latency, 122.18 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 344.99 us, 0.09% latency, 398.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 349.28 us, 0.09% latency, 393.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 347.85 us, 0.09% latency, 395.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.23 us, 0.09% latency, 398.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 118.49 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.13 ms, 1.10% latency, 268.5 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 816.35 us, 0.22% latency, 452.46 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 820.16 us, 0.22% latency, 450.36 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 816.35 us, 0.22% latency, 452.46 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 107.29 us, 0.03% latency, 210.13 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 303.03 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 297.31 us, 0.08% latency, 0.0 FLOPS, )
      )
      (13): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.04 ms, 2.95% latency, 151.71 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.64 ms, 1.24% latency, 122.19 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.18 us, 0.09% latency, 397.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 349.52 us, 0.09% latency, 393.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.18 us, 0.09% latency, 397.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 344.99 us, 0.09% latency, 398.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 118.49 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.1 ms, 1.10% latency, 269.99 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 816.11 us, 0.22% latency, 452.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 818.49 us, 0.22% latency, 451.28 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 816.82 us, 0.22% latency, 452.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 107.29 us, 0.03% latency, 210.13 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 301.12 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 300.41 us, 0.08% latency, 0.0 FLOPS, )
      )
      (14): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.08 ms, 2.96% latency, 151.19 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.68 ms, 1.25% latency, 121.18 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 347.38 us, 0.09% latency, 395.65 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 348.33 us, 0.09% latency, 394.57 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 347.85 us, 0.09% latency, 395.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 344.75 us, 0.09% latency, 398.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 119.92 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.11 ms, 1.10% latency, 269.8 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 818.97 us, 0.22% latency, 451.02 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 820.88 us, 0.22% latency, 449.97 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 819.21 us, 0.22% latency, 450.88 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 104.9 us, 0.03% latency, 214.9 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 302.31 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 299.45 us, 0.08% latency, 0.0 FLOPS, )
      )
      (15): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.02 ms, 2.95% latency, 151.95 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.64 ms, 1.24% latency, 122.23 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.23 us, 0.09% latency, 398.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 347.85 us, 0.09% latency, 395.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.9 us, 0.09% latency, 396.19 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.23 us, 0.09% latency, 398.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 118.97 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.13 ms, 1.10% latency, 268.41 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 816.82 us, 0.22% latency, 452.2 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 820.64 us, 0.22% latency, 450.1 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 818.73 us, 0.22% latency, 451.15 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 112.3 us, 0.03% latency, 200.76 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 303.75 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 300.17 us, 0.08% latency, 0.0 FLOPS, )
      )
      (16): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.02 ms, 2.95% latency, 152.0 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.64 ms, 1.24% latency, 122.14 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.71 us, 0.09% latency, 397.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 349.28 us, 0.09% latency, 393.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 352.14 us, 0.09% latency, 390.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.71 us, 0.09% latency, 397.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 124.22 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.12 ms, 1.10% latency, 268.88 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 817.78 us, 0.22% latency, 451.67 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 820.4 us, 0.22% latency, 450.23 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 818.49 us, 0.22% latency, 451.28 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 105.38 us, 0.03% latency, 213.93 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 304.46 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 298.98 us, 0.08% latency, 0.0 FLOPS, )
      )
      (17): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.06 ms, 2.96% latency, 151.43 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.63 ms, 1.24% latency, 122.38 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.47 us, 0.09% latency, 397.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 347.38 us, 0.09% latency, 395.65 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.9 us, 0.09% latency, 396.19 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.42 us, 0.09% latency, 396.74 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 118.73 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.12 ms, 1.10% latency, 269.11 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 816.58 us, 0.22% latency, 452.33 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 821.11 us, 0.22% latency, 449.84 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 817.3 us, 0.22% latency, 451.94 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 106.57 us, 0.03% latency, 211.54 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 302.55 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 298.98 us, 0.08% latency, 0.0 FLOPS, )
      )
      (18): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.1 ms, 2.97% latency, 150.89 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.67 ms, 1.25% latency, 121.3 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.71 us, 0.09% latency, 397.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 349.28 us, 0.09% latency, 393.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 347.38 us, 0.09% latency, 395.65 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.18 us, 0.09% latency, 397.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 122.55 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.11 ms, 1.10% latency, 269.83 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 816.35 us, 0.22% latency, 452.46 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 822.78 us, 0.22% latency, 448.92 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 816.58 us, 0.22% latency, 452.33 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 107.29 us, 0.03% latency, 210.13 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 302.79 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 298.98 us, 0.08% latency, 0.0 FLOPS, )
      )
      (19): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.13 ms, 2.98% latency, 150.53 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.64 ms, 1.24% latency, 122.25 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.18 us, 0.09% latency, 397.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.42 us, 0.09% latency, 396.74 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.95 us, 0.09% latency, 397.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.95 us, 0.09% latency, 397.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 118.26 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.14 ms, 1.11% latency, 267.39 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 818.73 us, 0.22% latency, 451.15 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 822.07 us, 0.22% latency, 449.32 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 818.25 us, 0.22% latency, 451.41 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 106.81 us, 0.03% latency, 211.07 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 302.08 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 299.93 us, 0.08% latency, 0.0 FLOPS, )
      )
      (20): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.04 ms, 2.95% latency, 151.67 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.65 ms, 1.24% latency, 121.95 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.71 us, 0.09% latency, 397.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 349.52 us, 0.09% latency, 393.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.9 us, 0.09% latency, 396.19 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 347.61 us, 0.09% latency, 395.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 118.49 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.13 ms, 1.10% latency, 268.27 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 821.35 us, 0.22% latency, 449.71 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 823.97 us, 0.22% latency, 448.27 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 816.35 us, 0.22% latency, 452.46 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 107.29 us, 0.03% latency, 210.13 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 303.27 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 299.93 us, 0.08% latency, 0.0 FLOPS, )
      )
      (21): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.68 ms, 3.12% latency, 143.44 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 5.25 ms, 1.40% latency, 107.95 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 344.75 us, 0.09% latency, 398.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 962.97 us, 0.26% latency, 142.72 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 349.04 us, 0.09% latency, 393.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.95 us, 0.09% latency, 397.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 118.26 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.13 ms, 1.10% latency, 268.54 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 817.54 us, 0.22% latency, 451.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 823.74 us, 0.22% latency, 448.4 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 817.3 us, 0.22% latency, 451.94 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 106.33 us, 0.03% latency, 212.01 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 303.98 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 299.45 us, 0.08% latency, 0.0 FLOPS, )
      )
      (22): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.15 ms, 2.98% latency, 150.19 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.65 ms, 1.24% latency, 122.03 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.18 us, 0.09% latency, 397.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 348.33 us, 0.09% latency, 394.57 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 348.33 us, 0.09% latency, 394.57 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.23 us, 0.09% latency, 398.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 119.21 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.13 ms, 1.10% latency, 268.61 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 816.35 us, 0.22% latency, 452.46 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 821.35 us, 0.22% latency, 449.71 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 817.54 us, 0.22% latency, 451.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 105.62 us, 0.03% latency, 213.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 305.65 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 301.12 us, 0.08% latency, 0.0 FLOPS, )
      )
      (23): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.01 ms, 2.94% latency, 152.14 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.63 ms, 1.24% latency, 122.48 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 344.28 us, 0.09% latency, 399.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 347.14 us, 0.09% latency, 395.92 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.9 us, 0.09% latency, 396.19 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.18 us, 0.09% latency, 397.01 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 120.4 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.11 ms, 1.10% latency, 269.3 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 821.35 us, 0.22% latency, 449.71 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 818.97 us, 0.22% latency, 451.02 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 815.63 us, 0.22% latency, 452.86 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 107.05 us, 0.03% latency, 210.6 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 300.65 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 301.36 us, 0.08% latency, 0.0 FLOPS, )
      )
      (24): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.15 ms, 2.98% latency, 150.28 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.7 ms, 1.26% latency, 120.64 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 347.61 us, 0.09% latency, 395.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 349.04 us, 0.09% latency, 393.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 348.09 us, 0.09% latency, 394.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.95 us, 0.09% latency, 397.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 119.69 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.13 ms, 1.10% latency, 268.49 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 815.39 us, 0.22% latency, 452.99 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 819.21 us, 0.22% latency, 450.88 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 815.63 us, 0.22% latency, 452.86 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 106.1 us, 0.03% latency, 212.49 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 300.17 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 298.02 us, 0.08% latency, 0.0 FLOPS, )
      )
      (25): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.11 ms, 2.97% latency, 150.71 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.66 ms, 1.25% latency, 121.65 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.47 us, 0.09% latency, 397.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 348.57 us, 0.09% latency, 394.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.95 us, 0.09% latency, 397.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.9 us, 0.09% latency, 396.19 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 118.97 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.12 ms, 1.10% latency, 268.83 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 821.83 us, 0.22% latency, 449.45 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 820.16 us, 0.22% latency, 450.36 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 815.63 us, 0.22% latency, 452.86 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 108.48 us, 0.03% latency, 207.82 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 300.88 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 298.02 us, 0.08% latency, 0.0 FLOPS, )
      )
      (26): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 11.52 ms, 3.08% latency, 145.46 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.68 ms, 1.25% latency, 121.1 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 346.9 us, 0.09% latency, 396.19 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 348.33 us, 0.09% latency, 394.57 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 347.85 us, 0.09% latency, 395.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.95 us, 0.09% latency, 397.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 120.64 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 4.54 ms, 1.21% latency, 244.13 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 817.06 us, 0.22% latency, 452.07 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 818.01 us, 0.22% latency, 451.54 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 817.54 us, 0.22% latency, 451.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 109.67 us, 0.03% latency, 205.56 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 301.12 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 299.45 us, 0.08% latency, 0.0 FLOPS, )
      )
      (27): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 9.11 ms, 2.44% latency, 183.91 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.46 ms, 1.19% latency, 127.24 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 344.04 us, 0.09% latency, 399.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 344.04 us, 0.09% latency, 399.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 340.94 us, 0.09% latency, 403.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 343.56 us, 0.09% latency, 400.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 113.25 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 3.38 ms, 0.90% latency, 327.86 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 817.3 us, 0.22% latency, 451.94 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 817.06 us, 0.22% latency, 452.07 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 813.01 us, 0.22% latency, 454.32 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 100.14 us, 0.03% latency, 225.14 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 300.17 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 299.69 us, 0.08% latency, 0.0 FLOPS, )
      )
      (28): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 8.79 ms, 2.35% latency, 190.53 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.14 ms, 1.11% latency, 136.85 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 343.08 us, 0.09% latency, 400.6 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 340.22 us, 0.09% latency, 403.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 339.27 us, 0.09% latency, 405.1 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.71 us, 0.09% latency, 397.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 113.96 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 3.37 ms, 0.90% latency, 328.63 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 817.06 us, 0.22% latency, 452.07 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 817.78 us, 0.22% latency, 451.67 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 812.53 us, 0.22% latency, 454.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 99.42 us, 0.03% latency, 226.76 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 301.6 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 299.93 us, 0.08% latency, 0.0 FLOPS, )
      )
      (29): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 8.81 ms, 2.36% latency, 190.13 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.16 ms, 1.11% latency, 136.36 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 344.28 us, 0.09% latency, 399.21 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 340.7 us, 0.09% latency, 403.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 340.46 us, 0.09% latency, 403.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 344.99 us, 0.09% latency, 398.38 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 113.73 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 3.37 ms, 0.90% latency, 328.82 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 816.11 us, 0.22% latency, 452.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 815.63 us, 0.22% latency, 452.86 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 813.96 us, 0.22% latency, 453.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 100.85 us, 0.03% latency, 223.54 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 300.65 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 299.22 us, 0.08% latency, 0.0 FLOPS, )
      )
      (30): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 8.85 ms, 2.37% latency, 189.37 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.14 ms, 1.11% latency, 136.85 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 349.52 us, 0.09% latency, 393.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 340.7 us, 0.09% latency, 403.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 340.7 us, 0.09% latency, 403.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 344.75 us, 0.09% latency, 398.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 113.01 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 3.42 ms, 0.92% latency, 323.71 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 816.11 us, 0.22% latency, 452.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 816.35 us, 0.22% latency, 452.46 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 813.01 us, 0.22% latency, 454.32 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 162.36 us, 0.04% latency, 138.85 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 300.41 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 299.93 us, 0.08% latency, 0.0 FLOPS, )
      )
      (31): LlamaDecoderLayer(
        8.19 k, 0.01% Params, 837.52 GMACs, 3.06% MACs, 8.78 ms, 2.35% latency, 190.75 TFLOPS, 
        (self_attn): LlamaAttention(
          0, 0.00% Params, 283.47 GMACs, 1.04% MACs, 4.14 ms, 1.11% latency, 136.87 TFLOPS, 
          (q_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 348.57 us, 0.09% latency, 394.3 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 342.61 us, 0.09% latency, 401.16 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 339.98 us, 0.09% latency, 404.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0, 0.00% Params, 68.72 GMACs, 0.25% MACs, 345.47 us, 0.09% latency, 397.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0, 0.00% Params, 0 MACs, 0.00% MACs, 112.3 us, 0.03% latency, 0.0 FLOPS, )
        )
        (mlp): LlamaMLP(
          0, 0.00% Params, 554.05 GMACs, 2.03% MACs, 3.36 ms, 0.90% latency, 329.77 TFLOPS, 
          (gate_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 815.63 us, 0.22% latency, 452.86 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 816.35 us, 0.22% latency, 452.46 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(0, 0.00% Params, 184.68 GMACs, 0.68% MACs, 817.54 us, 0.22% latency, 451.8 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation(0, 0.00% Params, 0 MACs, 0.00% MACs, 101.8 us, 0.03% latency, 221.45 GFLOPS, )
        )
        (input_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 300.41 us, 0.08% latency, 0.0 FLOPS, )
        (post_attention_layernorm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 298.98 us, 0.08% latency, 0.0 FLOPS, )
      )
    )
    (norm): LlamaRMSNorm(4.1 k, 0.00% Params, 0 MACs, 0.00% MACs, 300.17 us, 0.08% latency, 0.0 FLOPS, )
  )
  (lm_head): Linear(0, 0.00% Params, 536.87 GMACs, 1.96% MACs, 2.19 ms, 0.59% latency, 490.43 TFLOPS, in_features=4096, out_features=32000, bias=False)
)
------------------------------------------------------------------------------
